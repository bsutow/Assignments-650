{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggressive-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Good', 'Morning', 'I', 'hope', 'you', 'are', 'having', 'a', 'great', 'day', 'Professor', 'Please', 'give', 'me', 'an', 'A']\n"
     ]
    }
   ],
   "source": [
    "#Assignment 10.1A#\n",
    "\n",
    "import string\n",
    "\n",
    "def tokenize(sentence):\n",
    "    # Split the sentence by spaces\n",
    "    words = sentence.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    return stripped\n",
    "\n",
    "sentence = \"Good Morning, I hope you are having a great day Professor! Please give me an A+\"\n",
    "tokens = tokenize(sentence)\n",
    "print(type(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conventional-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Good',)\n",
      "('Morning',)\n",
      "('I',)\n",
      "('hope',)\n",
      "('you',)\n",
      "('are',)\n",
      "('having',)\n",
      "('a',)\n",
      "('great',)\n",
      "('day',)\n",
      "('Professor',)\n",
      "('Please',)\n",
      "('give',)\n",
      "('me',)\n",
      "('an',)\n",
      "('A',)\n"
     ]
    }
   ],
   "source": [
    "#Assignmet 10.1B#\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "def ngram(tokens, n):\n",
    "    # Split the sentence by spaces\n",
    "    sentence = tokens.split()\n",
    "    # Remove punctuation\n",
    "    newtable = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(newtable) for w in sentence]\n",
    "    ngrams = nltk.ngrams(stripped, n)\n",
    "    return ngrams\n",
    "\n",
    "paragraph = \"Fine Professor, I will take an A- instead! But, that is the lowest I am going!\"\n",
    "ngrams = ngram(sentence, 1)\n",
    "for g in ngrams:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guided-rabbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good' 'Morning' 'I' 'hope' 'you' 'are' 'having' 'a' 'great' 'day'\n",
      " 'Professor' 'Please' 'give' 'me' 'an' 'A']\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Assignment 10.1C#\n",
    "import string\n",
    "import nltk\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def one_hot_encode(tokens):\n",
    "    results = array(tokens)\n",
    "    print(results)\n",
    "    results = to_categorical(data)\n",
    "    return results\n",
    "\n",
    "data = [1,2,4]\n",
    "value = one_hot_encode(tokens)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alternate-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 50, 50)            25000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                80032     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 105,065\n",
      "Trainable params: 105,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dsc650/data/external/imdb/aclImdb/test/neg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fe70599bca4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dsc650/data/external/imdb/aclImdb/test/neg'"
     ]
    }
   ],
   "source": [
    "#Assignment 10.2#\n",
    "#Not sure why I am getting an error, as I have this file#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "imdb_dir = Path('dsc650/data/external/imdb/aclImdb')\n",
    "training_samples = 100\n",
    "maxlen = 50\n",
    "max_words = 500\n",
    "embedding_dim = 50\n",
    "max_features = 1000\n",
    "\n",
    "\n",
    "\n",
    "training_samples = 50\n",
    "validation_samples = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)\n",
    "\n",
    "\n",
    "\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "champion-career",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2a668f690d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m               metrics=['acc'])\n\u001b[0;32m---> 23\u001b[0;31m history = model.fit(input_train, y_train,\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1038\u001b[0m       \u001b[0;31m# `Tensor` and `NumPy` input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       (x, y, sample_weight), validation_data = (\n\u001b[0;32m-> 1040\u001b[0;31m           data_adapter.train_validation_split(\n\u001b[0m\u001b[1;32m   1041\u001b[0m               (x, y, sample_weight), validation_split=validation_split))\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1355\u001b[0m   \u001b[0munsplitable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_arrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_can_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0munsplitable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1358\u001b[0m         \u001b[0;34m\"`validation_split` is only supported for Tensors or NumPy \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \"arrays, found following types in the input: {}\".format(unsplitable))\n",
      "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>]"
     ]
    }
   ],
   "source": [
    "#Assignment 10.3#\n",
    "#It for some reason kept pulling up this error, I could not figure out how to solve it. From what I saw there is nothing online\n",
    "\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "\n",
    "input_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "input_test = data[training_samples: training_samples + validation_samples]\n",
    "y_test = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accessible-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 11s 66ms/step - loss: 0.8199 - acc: 0.5015 - val_loss: 0.6882 - val_acc: 0.5416\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.6737 - acc: 0.6379 - val_loss: 0.6736 - val_acc: 0.6246\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 0.6453 - acc: 0.7419 - val_loss: 0.6468 - val_acc: 0.6458\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 0.5970 - acc: 0.7941 - val_loss: 0.5612 - val_acc: 0.7774\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 9s 60ms/step - loss: 0.4938 - acc: 0.8330 - val_loss: 0.4497 - val_acc: 0.8254\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.3745 - acc: 0.8661 - val_loss: 0.4164 - val_acc: 0.8388\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.3169 - acc: 0.8856 - val_loss: 0.4039 - val_acc: 0.8540\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.2741 - acc: 0.9035 - val_loss: 0.3969 - val_acc: 0.8608\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.2525 - acc: 0.9095 - val_loss: 0.4198 - val_acc: 0.8620\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 9s 59ms/step - loss: 0.2260 - acc: 0.9236 - val_loss: 0.4186 - val_acc: 0.8692\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-reliance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
